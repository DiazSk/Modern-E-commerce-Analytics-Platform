@startuml E-Commerce Analytics Architecture

' ============================================
' STYLING AND THEME
' ============================================
!define LIGHTBLUE #E3F2FD
!define LIGHTORANGE #FFE0B2
!define LIGHTGREEN #E8F5E9
!define LIGHTPURPLE #C39BD3
!define LIGHTRED #FFCDD2
!define LIGHTGREY #EEEEEE

skinparam backgroundColor white
skinparam defaultTextAlignment center
skinparam ArrowThickness 2
skinparam ComponentStyle rectangle
skinparam defaultFontSize 14

title E-Commerce Analytics Platform Architecture

' ============================================
' LAYER 1: DATA SOURCES
' ============================================
package "DATA SOURCES" as sources LIGHTBLUE {
    component "REST API\n(FakeStore)\n200 SKUs" as api
    database "PostgreSQL\nOrders\nCustomers\nOrder Items" as postgres
    component "Event Stream\n(CSV Batches)\nClickstream\n50K+ events/day" as events
}

' ============================================
' LAYER 2: INGESTION (Apache Airflow)
' ============================================
package "INGESTION LAYER" as ingestion LIGHTORANGE {
    component "Apache Airflow\n(Docker)" as airflow {
        [DAG: Products API\nDaily Pull] as dag1
        [DAG: PostgreSQL\nIncremental Extract] as dag2
        [DAG: Clickstream\nBatch Landing] as dag3
    }
}

' ============================================
' LAYER 3: STORAGE (AWS S3 Data Lake)
' ============================================
package "STORAGE LAYER" as storage LIGHTGREEN {
    cloud "AWS S3 Data Lake" as s3 {
        folder "ecommerce-raw-data" as raw {
            [raw/products/YYYY/MM/DD/] as raw_products
            [raw/orders/YYYY/MM/DD/] as raw_orders
            [raw/events/YYYY/MM/DD/] as raw_events
        }
        
        note right of raw
            âœ“ Versioning Enabled
            âœ“ Encryption (AES256)
            âœ“ Lifecycle: 90dâ†’IA, 180dâ†’Glacier
            âœ“ 56% cost savings
        end note
    }
}

' ============================================
' LAYER 4: TRANSFORMATION (dbt + Snowflake)
' ============================================
package "TRANSFORMATION LAYER" as transformation LIGHTPURPLE {
    component "dbt (Data Build Tool)" as dbt {
        rectangle "STAGING\n(Bronze/Silver)" as staging {
            [stg_orders] as stg1
            [stg_customers] as stg2
            [stg_products] as stg3
            [stg_events] as stg4
        }
        
        rectangle "MARTS - CORE\n(Gold)" as marts_core {
            [dim_customers\n(SCD Type 2)] as dim1
            [dim_products] as dim2
            [dim_date] as dim3
            [fact_orders\n(Line Item Grain)] as fact
        }
        
        rectangle "MARTS - ANALYTICS" as marts_analytics {
            [customer_lifetime_value] as analytics1
            [product_performance] as analytics2
        }
    }
    
    database "Snowflake\nData Warehouse" as snowflake {
        note right
            Partitioned: order_date (day)
            Clustered: customer_key, product_key
            Query time: 4.2s â†’ 1.1s (74% faster)
            Data scanned: 85% reduction
        end note
    }
}

note right of dbt
    Data Loading Pattern:
    1. S3 â†’ Snowflake (COPY INTO)
    2. dbt transforms in Snowflake
end note

' ============================================
' LAYER 5: CONSUMPTION (Metabase BI)
' ============================================
package "CONSUMPTION LAYER" as consumption LIGHTRED {
    component "Metabase\n(BI Tool)" as metabase {
        [ðŸ“Š Executive Dashboard] as dash1
        [ðŸ“Š Customer Analytics] as dash2
        [ðŸ“Š Product Performance] as dash3
        [ðŸ“Š Funnel Analysis] as dash4
    }
}

' ============================================
' CROSS-CUTTING CONCERNS
' ============================================
package "DATA QUALITY" as quality LIGHTGREY {
    [Great Expectations\nâ€¢ Schema validation\nâ€¢ Null checks\nâ€¢ Referential integrity] as ge
}

package "MONITORING" as monitoring LIGHTGREY {
    [Airflow Alerts\ndbt Tests\nSnowflake Metrics] as alerts
}

package "INFRASTRUCTURE" as infra LIGHTGREY {
    [Terraform (IaC)\nDocker Compose\nGitHub Actions] as terraform
}

' ============================================
' DATA FLOW RELATIONSHIPS
' ============================================

' Sources to Ingestion
api -down-> dag1 : "Daily Batch Pull\n(REST)"
postgres -down-> dag2 : "Incremental Extract\n(SQL)"
events -down-> dag3 : "Batch Landing\n(CSV)"

' Ingestion to Storage
dag1 -down-> raw_products : "Raw Data\n(JSON)"
dag2 -down-> raw_orders : "Raw Data\n(CSV)"
dag3 -down-> raw_events : "Raw Data\n(CSV)"

' Storage to Transformation
raw -down-> staging : "SQL\nTransformations"

' Staging to Marts
staging -down-> marts_core
marts_core -down-> marts_analytics

' dbt to Snowflake
dbt -down-> snowflake : "Materialized\nTables/Views"

' Snowflake to Consumption
snowflake -down-> metabase : "SQL Queries"

' Cross-cutting to layers
ge -left-> staging : "Validate"
alerts -left-> airflow : "Monitor"
terraform -down-> s3 : "Provision"

' ============================================
' METRICS AND ANNOTATIONS
' ============================================
note bottom of sources
    <b>Data Volume:</b>
    â€¢ 5,000+ orders/day
    â€¢ 50,000+ events/day
    â€¢ 200 product SKUs
    â€¢ 1,000+ customers
end note

note bottom of consumption
    <b>Business Impact:</b>
    â€¢ Query Performance: 74% faster
    â€¢ Storage Cost: 56% savings
    â€¢ Data Quality: 99.8% accuracy
    â€¢ Processing Time: < 2 mins
end note

@enduml
