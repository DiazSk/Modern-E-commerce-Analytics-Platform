# Week 5: Optimization & Data Quality (Nov 17-23)

## ğŸ“‹ Overview
This week focuses on query performance optimization and implementing comprehensive data quality frameworks using both dbt native tests and Great Expectations.

## ğŸ¯ Learning Objectives
- Document query optimization strategies (partitioning, clustering)
- Measure and document performance improvements
- Implement Great Expectations for advanced data quality checks
- Create custom dbt tests for business logic validation
- Integrate data quality checks into Airflow pipelines

## ğŸ“… Implementation Timeline

### Day 1-2: Query Optimization
**Branch:** `feature/week5-query-optimization`

#### Tasks:
1. **Add Optimization Documentation to fact_orders.sql**
   - Document partitioning strategy (by order_date)
   - Document clustering strategy (by customer_key, product_key)
   - Include performance metrics (before/after comparison)
   - Document cost savings analysis

2. **Create Performance Test Queries**
   - Create `tests/performance/` directory
   - Write benchmark queries for common access patterns
   - Document execution times and data scanned
   - Compare before/after optimization metrics

#### Deliverables:
- âœ… Enhanced fact_orders.sql with optimization comments
- âœ… Performance test query file
- âœ… Documentation of performance improvements

### Day 3-5: Great Expectations Integration
**Branch:** `feature/week5-great-expectations`

#### Tasks:
1. **Install and Initialize Great Expectations**
   ```bash
   pip install great-expectations
   great_expectations init
   ```

2. **Create Expectation Suite**
   - Create `scripts/create_expectations.py`
   - Define expectations for fact_orders:
     - Row count validation
     - Null checks on critical columns
     - Unique key constraints
     - Value range validations
     - Status value set validation
     - Email format regex validation

3. **Integrate with Airflow**
   - Create `dags/data_quality_checks.py`
   - Implement run_data_quality_checks() function
   - Set up checkpoint execution
   - Configure failure handling

#### Deliverables:
- âœ… Great Expectations configuration
- âœ… Expectation suite for fact_orders
- âœ… Airflow DAG for data quality validation
- âœ… Documentation on running quality checks

### Day 6-7: Enhanced dbt Tests
**Branch:** `feature/week5-dbt-tests`

#### Tasks:
1. **Create Custom dbt Tests**
   - Create `tests/assert_positive_order_total.sql`
   - Test business logic: line_total = quantity * unit_price - discount
   - Validate data consistency across models

2. **Enhance schema.yml with dbt_expectations**
   - Add expression_is_true tests for calculated fields
   - Add relationship tests for foreign keys
   - Add accepted_values tests for status columns
   - Add not_null and unique constraints

3. **Run and Document Tests**
   ```bash
   dbt test
   dbt test --select fact_orders
   dbt test --select tag:quality
   ```

#### Deliverables:
- âœ… Custom test suite in tests/ directory
- âœ… Enhanced schema.yml with comprehensive tests
- âœ… Test execution documentation
- âœ… Test failure analysis and remediation guide

## ğŸ—ï¸ Architecture Components

### Directory Structure
```
transform/
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ performance/
â”‚   â”‚   â””â”€â”€ query_performance.sql
â”‚   â””â”€â”€ assert_positive_order_total.sql
â”œâ”€â”€ models/marts/core/
â”‚   â”œâ”€â”€ fact_orders.sql (enhanced with optimization docs)
â”‚   â””â”€â”€ schema.yml (enhanced with dbt_expectations tests)
â””â”€â”€ great_expectations/
    â”œâ”€â”€ expectations/
    â”‚   â””â”€â”€ orders_suite.json
    â””â”€â”€ checkpoints/
        â””â”€â”€ orders_checkpoint.yml

scripts/
â””â”€â”€ create_expectations.py

dags/
â””â”€â”€ data_quality_checks.py
```

## ğŸ“Š Key Concepts

### Query Optimization Strategies

#### 1. Partitioning
- **What**: Physically dividing data into smaller, manageable chunks
- **Why**: Reduces data scanned for date-filtered queries
- **How**: Partition by order_date with day granularity
- **Impact**: 85% reduction in data scanned

#### 2. Clustering
- **What**: Organizing data within partitions by frequently accessed columns
- **Why**: Improves query performance for joins and filters
- **How**: Cluster by customer_key and product_key
- **Impact**: 74% reduction in query execution time

### Data Quality Framework

#### Great Expectations
- **Purpose**: Advanced, Python-based data validation
- **Strengths**:
  - Complex validation rules
  - Rich reporting and documentation
  - Integration with various data sources
  - Statistical profiling capabilities

#### dbt Tests
- **Purpose**: SQL-based data quality tests
- **Strengths**:
  - Integrated with dbt workflow
  - Version controlled with models
  - Fast execution within dbt pipeline
  - Easy to write and maintain

## ğŸ¯ Success Metrics

### Performance Improvements
- Query execution time: **4.2s â†’ 1.1s** (74% improvement)
- Data scanned per query: **1.2GB â†’ 180MB** (85% reduction)
- Daily cost savings: **~$5/day** for 1000 queries

### Data Quality Coverage
- **100% test coverage** on critical business logic
- **Zero tolerance** for null values in primary keys
- **Automated validation** on every data load
- **Clear failure reporting** with actionable insights

## ğŸ”— Integration Points

### With Previous Weeks
- **Week 3**: Uses dbt models created (staging, marts)
- **Week 4**: Optimizes dimensional models (fact_orders, dim_*)

### With Future Weeks
- **Week 6**: Quality checks will be part of production deployment
- Performance optimizations inform infrastructure scaling decisions

## ğŸ“ Interview Talking Points

### Query Optimization
- "Implemented partitioning strategy that reduced query costs by 85%"
- "Documented performance improvements using before/after benchmarks"
- "Applied clustering based on query access patterns analysis"

### Data Quality
- "Built comprehensive data quality framework using dual approach: dbt tests for structural validation and Great Expectations for complex business rules"
- "Integrated quality checks into Airflow pipeline with automated failure alerts"
- "Achieved 100% test coverage on critical data paths"

### Tools & Technologies
- Great Expectations for advanced data validation
- dbt native tests and dbt_expectations package
- Airflow for orchestrating quality checks
- SQL query optimization techniques

## ğŸš€ Getting Started

1. **Create feature branches:**
   ```bash
   # Windows
   scripts\create_week5_branches.bat

   # Linux/Mac
   bash scripts/create_week5_branches.sh
   ```

2. **Start with Query Optimization:**
   ```bash
   git checkout feature/week5-query-optimization
   ```

3. **Follow the day-by-day implementation plan**

4. **Merge to develop after completing each feature**

## ğŸ“š Resources
- [Great Expectations Documentation](https://docs.greatexpectations.io/)
- [dbt Testing Best Practices](https://docs.getdbt.com/docs/building-a-dbt-project/tests)
- [dbt Expectations Package](https://github.com/calogica/dbt-expectations)
- [Query Optimization Techniques](https://docs.snowflake.com/en/user-guide/tables-clustering)

## âš ï¸ Important Notes
- Always benchmark before claiming performance improvements
- Document all test failures and remediation steps
- Keep optimization comments up-to-date with code changes
- Great Expectations requires Snowflake connection for integration tests
- dbt tests run during `dbt test` command, not during `dbt run`

## ğŸ“ Resume Bullets

Ready-to-use resume bullets for this week:

- **Query Optimization**: "Optimized analytical queries by implementing partitioning and clustering strategies, achieving 74% improvement in execution time (4.2s â†’ 1.1s) and 85% reduction in data scanned (1.2GB â†’ 180MB)"

- **Data Quality Framework**: "Designed and implemented comprehensive data quality framework using Great Expectations and dbt tests, achieving 100% test coverage on critical business logic with automated validation in Airflow pipelines"

- **Cost Optimization**: "Reduced data warehouse query costs by $5/day through strategic partitioning and clustering optimization, documented with performance benchmarks"

- **Test Automation**: "Created custom dbt tests and Great Expectations suites for validating business rules, data integrity, and referential constraints across 4 dimensional models and 1 fact table"
